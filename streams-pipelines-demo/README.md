# üåä Node.js Streams & Pipelines - Real World Examples

‡§Ø‡§π project Node.js Streams ‡§î‡§∞ Pipelines ‡§ï‡•ã practically demonstrate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à real-world examples ‡§ï‡•á ‡§∏‡§æ‡§•‡•§

## üìö Table of Contents

1. [Streams ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡§Ç?](#streams-‡§ï‡•ç‡§Ø‡§æ-‡§π‡•à‡§Ç)
2. [Types of Streams](#types-of-streams)
3. [Why Use Streams?](#why-use-streams)
4. [Examples](#examples)
5. [Installation](#installation)
6. [How to Run](#how-to-run)

---

## üéØ Streams ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡§Ç?

Streams Node.js ‡§Æ‡•á‡§Ç **data ‡§ï‡•ã chunks ‡§Æ‡•á‡§Ç process ‡§ï‡§∞‡§®‡•á** ‡§ï‡§æ ‡§§‡§∞‡•Ä‡§ï‡§æ ‡§π‡•à‡•§ ‡§¨‡§°‡§º‡•Ä files ‡§Ø‡§æ data ‡§ï‡•ã memory ‡§Æ‡•á‡§Ç load ‡§ï‡§ø‡§è ‡§¨‡§ø‡§®‡§æ process ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

### Key Benefits:
- ‚úÖ **Memory Efficient**: ‡§™‡•Ç‡§∞‡•Ä file memory ‡§Æ‡•á‡§Ç load ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•Ä
- ‚úÖ **Fast**: Data chunks ‡§Æ‡•á‡§Ç process ‡§π‡•ã‡§§‡§æ ‡§π‡•à
- ‚úÖ **Scalable**: Large files handle ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
- ‚úÖ **Composable**: Pipelines ‡§¨‡§®‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç

---

## üîÑ Types of Streams

### 1. **Readable Stream** üìñ
- Data read ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Examples: `fs.createReadStream()`, HTTP request

### 2. **Writable Stream** ‚úçÔ∏è
- Data write ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Examples: `fs.createWriteStream()`, HTTP response

### 3. **Transform Stream** üîÑ
- Data read ‡§ï‡§∞‡§ï‡•á transform ‡§ï‡§∞‡§ï‡•á write ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Examples: Compression, Encryption, Data transformation

### 4. **Duplex Stream** ‚ÜîÔ∏è
- Readable ‡§î‡§∞ Writable ‡§¶‡•ã‡§®‡•ã‡§Ç
- Examples: TCP sockets, WebSockets

---

## üí° Why Use Streams?

### Without Streams (Bad):
```javascript
// ‚ùå Entire file loaded in memory
const data = fs.readFileSync('large-file.txt'); // 1GB file = 1GB RAM!
processData(data);
```

### With Streams (Good):
```javascript
// ‚úÖ File processed in chunks
fs.createReadStream('large-file.txt')
  .pipe(transformStream)
  .pipe(fs.createWriteStream('output.txt'));
// Only small chunks in memory!
```

---

## üìÅ Examples

### 1. Basic Streams (`01-basic-streams.js`)
```bash
npm run basic
```
- Readable, Writable, Transform streams
- Basic stream operations
- Understanding stream events

### 2. Pipelines (`02-pipelines.js`)
```bash
npm run pipeline
```
- `pipeline()` function usage
- Error handling in pipelines
- Multiple stream chaining

### 3. File Streaming (`03-file-streaming.js`)
```bash
npm run file
```
- Reading large files
- Writing files with streams
- File copy with streams

### 4. Transform Streams (`04-transform-streams.js`)
```bash
npm run transform
```
- Custom transform streams
- Data transformation
- Chunk processing

### 5. CSV Processing (`05-csv-processing.js`)
```bash
npm run csv
```
- CSV parsing with streams
- Large CSV file processing
- Data transformation

### 6. HTTP Streaming (`06-http-streaming.js`)
```bash
npm run http
```
- HTTP request/response streaming
- Downloading files
- Streaming API responses

### 7. Large File Handling (`07-large-file-handling.js`)
```bash
npm run large-file
```
- Processing very large files
- Memory-efficient operations
- Progress tracking

### 8. Log Processing (`08-log-processing.js`)
```bash
npm run log
```
- Reading log files
- Filtering and processing logs
- Real-time log streaming

### 9. Backpressure Handling (`09-backpressure-handling.js`)
```bash
npm run backpressure
```
- Understanding backpressure
- Handling slow consumers
- Flow control

---

## üöÄ Installation

### 1. Install Dependencies:
```bash
npm install
```

### 2. Prepare Test Data (Optional):
- Some examples create test files automatically
- For CSV example, test data is generated
- For file examples, sample files are created

### 3. Run Examples:
```bash
npm run basic
npm run pipeline
# ... etc
```

---

## üöÄ How to Run

### Run Individual Examples:
```bash
npm run basic
npm run pipeline
npm run file
npm run transform
npm run csv
npm run http
npm run large-file
npm run log
npm run backpressure
```

### Run All Examples:
```bash
npm run all
```

### Or Directly:
```bash
node 01-basic-streams.js
node 02-pipelines.js
# ... etc
```

---

## üîë Key Concepts

### 1. **Stream Events**

```javascript
stream.on('data', (chunk) => {
  // Data chunk received
});

stream.on('end', () => {
  // Stream finished
});

stream.on('error', (err) => {
  // Error occurred
});
```

### 2. **Pipelines**

```javascript
import { pipeline } from 'stream/promises';

await pipeline(
  readableStream,
  transformStream,
  writableStream
);
```

### 3. **Backpressure**

- Fast producer + Slow consumer = Backpressure
- Streams automatically handle this
- `pause()` and `resume()` for manual control

### 4. **When to Use Streams**

‚úÖ **Use for:**
- Large files
- Network data
- Real-time data processing
- Data transformation pipelines
- Log processing

‚ùå **Don't use for:**
- Small files (< 1MB)
- Simple operations
- When you need entire data in memory

---

## üí° Important Takeaways

1. ‚úÖ Streams **memory efficient** ‡§π‡•à‡§Ç
2. ‚úÖ **Large files** handle ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
3. ‚úÖ **Pipelines** compose ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
4. ‚úÖ **Backpressure** automatically handle ‡§π‡•ã‡§§‡§æ ‡§π‡•à
5. ‚úÖ **Real-time processing** possible ‡§π‡•à

---

## üõ†Ô∏è Technologies Used

- **Node.js Streams API** (Native)
- **csv-parse** (CSV parsing)
- **csv-stringify** (CSV generation)
- **fs module** (File operations)
- **http module** (HTTP streaming)

---

## üìñ Further Reading

- [Node.js Streams Official Docs](https://nodejs.org/api/stream.html)
- [Streams Handbook](https://github.com/substack/stream-handbook)

---

**Happy Learning! üéì**

